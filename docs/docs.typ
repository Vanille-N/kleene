#import "@preview/mantys:1.0.2": *
#import "@preview/swank-tex:0.1.0": LaTeX
#import "@preview/tidy:0.4.3"

#import "revised.typ"

#show regex("Introduced in "): "Since "
#show regex("Available until "): "Until "
#show regex("Changed in "): "Changed "

#let repo = "https://github.com/Vanille-N/kleene/"
#let typst-repo = "https://github.com/typst/typst/"

#let property-unstable() = elements.note(
  dy: -2em,
  styles.pill(
    "emph.deprecated",
    (
      sym.arrow.zigzag + sym.space.nobreak + "Unstable"
    ),
  ),
)

#let show-module(name, module: false, scope: (:), outlined: true) = {
  let path = "../src/" + name + ".typ"
  import path as mod
  tidy-module(
    name,
    module: if module == true { name } else if module == false { none } else { module },
    read(path),
    scope: (
      property-unstable: property-unstable,
      ..scope,
    ),
  )
}

#let twocols(frac: 50%, l, r) = {
  table(columns: (frac, 100% - frac), stroke: none, l, r)
}

#show: revised.highlight-outline-entries

// @scrybe(skip 1; grep {{version}})
#let versions = (
  "0.1.0",
)

#show: mantys(
  ..toml("../typst.toml"),
  title: "KLEENE",
  subtitle: "User guide",
  date: datetime.today(),

  show-index: false,
  show-urls-in-footnotes: false,

  abstract: [
    KLEENE is a parser combinator based on Parsing Expression Grammars (PEG),
    that places a particular focus on *modularity*, *quality of error messages*,
    and *unit testing*. Parsers generated by KLEENE can be modified on the fly
    or extended with new rules, have built-in well-formatted error reporting,
    and are easy to test.

    KLEENE is *not* focused on performance above all else: PEGs interpreted by
    recursive descent are known to exhibit exponential-time worst-case parsing.
    If the parsing work you need is very time-sensitive, consider using a more
    specialized library.

    #v(5cm)

    *Contributions* \
    If you have ideas for improvements, or if you encounter a bug,
    you are encouraged to contribute to KLEENE by submitting a
    #link(repo + "issues")[bug report],
    #link(repo + "issues")[feature request],
    or #link(repo)[pull request].
    This includes submitting test cases.

    *Versions*
    - #link(repo)[`dev`]
    #{
      for (i, ver) in versions.enumerate() [
        - #link(repo + "releases/tag/v" + ver)[#raw(ver)]
          #{if i == 0 {
            [(#link("https://typst.app/universe/package/kleene")[`latest`])]
          }}
      ]
    }
    - #link(repo + "releases/")[...]

    #colbreak()
    /*
    #place(box(width: 100%, height: 100%)[
      #place(bottom + right, dx: 1.5cm)[
        #box(width: 6.6cm, stroke: gray, radius: 5mm, inset: 5mm)[
          #set text(fill: gray, size: 10.5pt)
          #set linebreak(justify: true)
          #show: align.with(left)
          Highlighted chapters denote
          #(revised.styles.breaking)[breaking changes],
          #(revised.styles.major)[major updates],
          #(revised.styles.minor)[minor updates],
          and #(revised.styles.new)[new additions],
          in the latest version #raw(versions.at(0))
        ]
      ]
    ])
    */
    #colbreak()
  ],
)

= Quick start

== Skeleton

At minimum, using KLEENE takes the following form:

#codesnippet[
  // @scrybe(jump import; grep preview; grep {{version}})
  ```typ
  #import "@preview/kleene:0.1.0"

  #let grammar = {

    // kleene.prelude contains all the operators, which we usually
    // don't want to have polluting the global namespace.
    import kleene.prelude: *

    kleene.grammar(
      // define the rules here
      main: {
        ..
      },
      ..
    )
  }

  // If the grammar includes any unit tests, this will evaluate them.
  #kleene.test(grammar)

  // This is how to invoke the parser with <main> as the entry point.
  #kleene.parse(grammar, <main>, "..")
  ```
]

#let test-example(code, post: auto) = {
  let pre = ```
    #import "/src/lib.typ" as kleene
    #import kleene.prelude: *
    ```
  if post == auto {
    post = ```
    #kleene.test(grammar, total: false)
    ```
  } else if post == none {
    post = ``
  }
  deps.codly.codly-disable()
  block(breakable: true, stroke: blue + 0.5pt, width: 100%, inset: 3mm, radius: 2mm)[
    #code
  ]
  block(breakable: true, stroke: gray, inset: 3mm, width: 100%, radius: 2mm)[
    #set text(size: 8pt)
    #eval(
      pre.text + "\n" + code.text + "\n" + post.text,
      mode: "markup",
    )
  ]
}

For example, here is a very simple grammar that will simply look for the
string `"foo"`:

#test-example(```typ
  #let grammar = kleene.grammar(
    // This defines a rule <main>
    main: {
      // Each `pat` gives one possible form of the match, here the
      // literal string "main"
      pat("foo")
    }
  )

  // In case of success, the result is a pair (true, value)
  #let (ok, ans) = kleene.parse(grammar, <main>, "foo")
  #assert(ok)
  #ans

  // In case of failure, the result is a pair (false, error-message)
  #let (ok, ans) = kleene.parse(grammar, <main>, "bar")
  #assert(not ok)
  #ans
  ```, post: none)

== Simple examples

=== Decimal integers

To start with an easy example, let's write a parser of integers in base 10.
We take the opportunity to demonstrate the use of `kleene.test` to run the
inline unit tests declared by `yy` and `nn`, as well as `tr` to transform
the parsed result post-matching.

#test-example(```typ
  #let grammar = kleene.grammar(
    digit: {
      pat(`[0-9]`)
    },
    digits: {
      // Using the <label> notation you can recursively refer to other rules
      pat(iter(<digit>))
      // `rw`, standing for "rewrite" applies a post-parsing transformation to the data.
      // By default, `repeat` produces an array.
      rw(ds => ds.join())
    },
    int: {
      pat(<digits>)
      rw(ds => int(ds))
      // The command `yy` adds positive examples
      yy(`42`, `4096`)
      // The command `nn` adds negative examples
      nn(`42b`, ``)
    },
  )
  ```)

=== Integers

KLEENE can backtrack, and the backtracking points are indicated by the operator
`fork`. Several possible sub-patterns can be given, and they will be explored
sequentially until a match. This lets you define a rule as the union of other
rules. To illustrate this, we add hexadecimal numbers to our parser.

#test-example(```typ
#let grammar = kleene.grammar(
  decdigit: pat(`[0-9]`),
  hexdigit: {
    // `fork` introduces a backtracking point
    pat(fork(`[a-f]`, `[A-F]`))
    // multiple `pat` invocations also implicitly induce a `fork`
    pat(<decdigit>)
  },
  decint: {
    pat(iter(<decdigit>))
    rw(ds => int(ds.join()))
  },
  hexint: {
    // If some part of the input is matched but not used,
    // such as here the prefix "0x", we indicate this with a `drop`
    // and it will be removed before the transformation is applied.
    pat(drop("0x"), iter(<hexdigit>))
    rw(ds => ds.flatten().join())
  },
  value: {
    pat(<hexint>)
    rw(i => (hex: i)) // a `rw` applies to all the `pat` that come before
    pat(<decint>)
    rw(i => (int: i))
    yy(`42`, `0xfae`)
  },
  value-bad: {
    // Note that the order of rules can be significant.
    // Here, a <hexint> has as prefix a <decint> (0),
    // so putting them in the wrong order will cause problems because
    // the parser will think it has a successful <decint> match and then
    // stop before the end.
    pat(fork(<decint>, <hexint>))
    nn(`0xfae`)
  },
)
```)

= Operators

== Constants

=== EOF <pat-eof>

```typc eof()``` matches the end of the stream and returns ```typc none```.

#test-example(```typ
#let grammar = kleene.grammar(
  main: {
    pat(eof())
    yy(``)
    nn(`x`)
  },
)  
```)

=== Str <pat-str>

```typc str("..")``` matches the literal string ```typc ".."``` and returns the match.
Any #typ.t.string is implicitly cast to `str`.

#test-example(```typ
#let grammar = kleene.grammar(
  main: {
    pat("foo")
    yy(`foo`)
    nn(`bar`)
  },
)
```)

=== Regex <pat-regex>

```typc regex("..")```, with ```typc ".."``` the same string you would pass to the standard
function ```typc std.regex```, matches a regular expression.
Any raw text (i.e., wrapped in ```typ `..` ```) is implicitly cast to `raw`.

#test-example(```typ
#let grammar = kleene.grammar(
  main: {
    pat(`[a-zA-Z]+`)
    yy(`Word`)
    nn(`w0rd`)
  }
)
```)

=== Label <pat-label>

```typc label("lab")``` recursively matches the rule named `lab`, which
must be defined in the same grammar.
Any #typ.t.label is implicitly cast to `label`.

#test-example(```typ
#let grammar = kleene.grammar(
  sub: pat("foo"),
  main: {
    pat(<sub>)
    yy(`foo`)
  },
)
```)

== Sequencing and branching

=== Seq <pat-seq>

```typc seq(pattern1, pattern2, ..)``` matches the successive patterns `pattern1`, `pattern2`, ...
one after the other. Any #typ.t.array is implicitly cast to a `seq`.
Furthermore most other operators (`iter`, `star`, `maybe`, `drop`, `rewrite`, ...)
are variadic and accept multiple arguments that are understood as a `seq`.
#info-alert[
  An *explicit* call to `seq` will always return an array, even if it has
  only one element. Implicit calls however, such as those induced by
  variadic `pat`, `iter`, ... may for convenience return a single element
  rather than a singleton array.
  In both cases the array returned by `seq` is pre-filtered to exclude elements
  that are `none` (either ```typc eof()``` matches, or those that were dropped
  as explained below).
]

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => int(ds))
  },
  pair: {
    pat("(", <int>, ",", <int>, ")")
    yy(`(42,64)`)
    nn(`(42,`)
  },
)
```)

=== Fork <pat-fork>

```typc fork(pattern1, pattern2, ..)``` is the standard dual of `seq`, returning
the first of `pattern`, `pattern2`, ... that has a successful match.

#warning-alert[
  `fork` always returns the *first match* in the order provided,
  rather than the longest match. As such `fork` does not commute.
]
#info-alert[
  `fork` is the only operator that may return objects of different
  types. It is recommended that you insert the appropriate `rw` calls
  to be able to identify the provenance of the result.
]

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    // The rewrites will help us identify the provenance of the match
    rw(ds => (int: int(ds)))
  },
  float: {
    pat(`[0-9]+`, ".", `[0-9]+`)
    rw(ds => (float: float(ds.flatten().join())))
  },
  number: {
    pat(fork(<float>, <int>))
    yy(`42`, `12.1`)
  },
  // Beware: when a rule is a valid prefix of another,
  // swapping them in the `fork` may lead to failures.
  number-bad: {
    pat(fork(<int>, <float>))
    nn(`12.1`)
  }
)
```)


== Repetitions

=== Maybe <pat-maybe>

```typc maybe(pattern)``` returns either a match of `pattern` or a unit ```typc ()```,
i.e. 0 or 1 matches of another pattern. If `maybe` is given multiple parameters,
they will be wrapped in a `seq`.
It is equivalent to `?` in regular expressions.

#info-alert[
  In the case of a match, the result will be wrapped in a singleton array.
  This follows a design convention of KLEENE that a given operator should always return
  an object of the same type.
  It is further motivated by the observed usage of `maybe`,
  which is commonly to match one more optional element of an already repeating sequence:
  it is much easier to append to an array an optional element `elt` when it is presented
  as `(elt,)` or `()` than if it were as `elt` or `()`.
]

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => int(ds))
  },
  optional: {
    pat(maybe(<int>))
    yy(`42`, ``)
  }
)
```)

=== Iter <pat-iter>

```typc iter(pattern)``` matches one or more repetitions of `pattern`.
If `iter` is given multiple parameters, they will be wrapped in a `seq`.
It is equivalent to `+` in regular expressions.

#info-alert[
  The result of an `iter` is always an array, even if there is only one match.
]

#test-example(```typ
#let grammar = kleene.grammar(
  digit: pat(`[0-9]`),
  int: {
    pat(iter(<digit>))
    rw(ds => int(ds.join()))
    yy(`420`)
    nn(``)
  }
)
```)

#info-alert[
  `iter` is sophisticated enough to not get trapped in an infinite loop if you give a pattern
  that risks consuming no input. In that case it will stop after one iteration.
  #test-example(```typ
  #let grammar = kleene.grammar(
    emp: {
      pat("[", star(""), "]")
      yy(`[]`)
    },
  )
  ```)
]

=== Star <pat-star>

```typc star(pattern)``` matches arbitrarily many repetitions of `pattern`, including 0.
If `star` is given multiple parameters, they will be wrapped in a `seq`.
It is equivalent to `*` in regular expressions.

#info-alert[
  The result of a `star` is always an array, even if there is only one match.
]

#test-example(````typ
#let grammar = kleene.grammar(
  whitespace: {
    pat(star(fork(" ", "\n", "\t")))
    rw(none)
  },
  comment: {
    pat("//", star(`[^\n]`), "\n")
    pat("/*", star(`[^*]`), "*/")
    rw(none)
  },
  irrelevant: {
    pat(star(fork(<comment>, <whitespace>)))
    rw(none)
    yy(```
       
      // a comment

      //
        /* another */  

     
       
    ```)
  }
)
````)

== Ghost operators

These operators do not consume text or produce outputs, but they can manipulate
how the parser behaves, transform data, and sometimes improve error messages.

=== Drop <pat-drop>

```typc drop(pattern)``` matches `pattern` but does not include its outcome in the result.
It can be useful to have less tuple unpacking in `rw`.
If `drop` receives multiple patterns it will implicitly wrap them in a `seq`.

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => int(ds))
  },
  tuple: {
    pat(drop("("), star(<int>, drop(",")), maybe(<int>), drop(")"))
    rw(l => l.flatten())
    yy(`()`, `(1,)`, `(1,2,3)`, `(1,2,3,)`)
  },
)
```)

=== Rewrite <pat-rewrite>

```typc rewrite(fun)(pattern)``` matches `pattern` then applies the transformation `fun`.
This can also be achieved with a `rw`, but `rewrite` is inlined.
If multiple patterns are provided, they are implicitly wrapped in a `seq`.

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(rewrite(ds => (int: int(ds)))(`[0-9]+`))
    yy(`42`)
  },
)
```)

=== Commit <pat-commit>

```typc commit()```, also written ```typ $$```, blocks backtracking.
This can be very useful to improve both the performance of the parser
and the quality of error messages.

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => (int: int(ds)))
  },
  hex: {
    pat(drop("0x"), `[0-9A-Fa-f]+`)
    rw(ds => (hex: ds))
  },
  val: {
    pat(fork(<hex>, <int>))
    yy(`42`, `0xFF`)
    nn(`0xZ`)
  },
)
```)
First we try to parse a ```typc <hex>```, but this fails due to ```typc "z"```
so the `fork` goes to the second branch where we try to pars an ```typc <int>```.
This also fails, but for a different reason. The error message of `fork`
takes the last failure, i.e. the one where we fail to parse an ```typc <int>```.

Consider instead an alternative that uses a commit point to block the backtracking
once we've read the hexadecimal prefix ```typc "0x"```:
#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => (int: int(ds)))
  },
  hex: {
    // Once we see "0x", we know it's supposed to be a hexadecimal number.
    pat(drop("0x"), $$, `[0-9A-Fa-f]+`)
    rw(ds => (hex: ds))
  },
  val: {
    pat(fork(<hex>, <int>))
    yy(`42`, `0xFF`)
    nn(`0xZ`)
  },
)
```)
The new error message much more accurately pinpoints the issue.

=== Try <pat-try>

```typc try(pattern)``` is the inverse of ```typc commit()```, restoring the ability
to backtrack.

== Lookaheads

Lookaheads perform a match, but do not actually consume the input stream.
This is only useful when the lookahead is followed by a pattern
that actually does consume input, and the lookahead only serves as a guard.

#warning-alert[
  As usual, lookaheads come with advantages and drawbacks depending on
  if you use them correctly. They might improve your error messages
  and reduce the amount of backtracking required,... or they might lead
  to worst-case exponential-time parsing.
]


=== Peek <pat-peek>

```typc peek(pat)``` introduces a positive lookahead: it will try to match `pat`,
but even if it succeeds it will not consume any input.
If multiple patterns are provided, they are implicitly cast to a @cmd:prelude:seq.
For example here is an example where a lookahead determines if we parse
a string as a key or as a value:

#test-example(```typ
#let grammar = kleene.grammar(
  string: pat(drop("\""), `[^"]*`, drop("\"")),
  ident: pat(`[a-zA-Z][a-zA-Z0-9]*`),
  named-arg: {
    pat(fork(<string>, <ident>), drop(` *: *`), <string>)
    rw(((k,v),) => (key: k, val: v))
    yy(`foo: "foo"`)
    yy(`"bar": "bar"`)
  },
  pos-arg: {
    pat(<string>)
    rw(v => (val: v))
    yy(`"baz"`)
  },
  arg: {
    // Read as: once we know that there is a ":" ahead,
    // we commit to parsing a <named-arg> without possibility
    // of backtracking anymore.
    pat(peek(`[^:,]*:`), $$, <named-arg>)
    pat(<pos-arg>)
    yy(`foo: "foo"`, `"bar": "bar"`, `"baz"`)
    nn(`"foo":`, `foo:`)
  },
  arg-worse: {
    // Contrast the quality of the error message above
    // compared to if we omit the lookahead:
    pat(<named-arg>)
    pat(<pos-arg>)
    nn(`"foo":`, `foo:`)
  },
)
```)

=== Neg <pat-neg>

Symmetrically ```typc neg(pat)``` will perform a negative lookahead,
succeeding without consuming input when the inner pattern fails.
If multiple patterns are provided, they are implicitly cast to a @cmd:prelude:seq.

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(neg("0x"), `[0-9]`, $$, `[0-9]*`)
    rw(ds => (int: int(ds.flatten().join())))
  },
  hex: {
    pat(drop("0x"), `[0-9a-fA-F]+`)
    rw(hex => (hex: hex))
  },
  val: {
    pat(fork(<int>, <hex>))
    yy(`42`, `0`, `0x31`)
    nn(`0b1`)
  }
)
```)

= Unit tests <tests>

Many have already been included in previous examples, but to reiterate:
- ```typc yy(input1, input2, ..)``` declares as many *positive* unit tests
  as there are inputs, i.e. they should succeed.
- ```typc nn(input1, input2, ..)``` declares as many *negative* unit tests
  as there are inputs, i.e. they should fail.

== Post-parsing validation <validation>

In addition to checking that positive tests parse and negative tests do not,
you may specify an additional `validate` parameter in the form of a function
that will receive the input of the test and the parsed output.
This function should return `none` if the result is valid, and anything else
will cause the test to fail.

#test-example(post: ``, ```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => (int: int(ds)))
  },
  hex: {
    pat(drop("0x"), `[0-9A-Fa-f]+`)
    rw(ds => (hex: ds))
  },
  val: {
    pat(fork(<hex>, <int>))
    yy(`0`)
    yy(`42`, `0xFF`, `0xZ`,
      validate: (_,ans) => if "int" not in ans [Integer expected])
  },
)

#kleene.test(grammar)
```)

A tally of all test outcomes is added to the bottom, showing from left to right
- the total number of tests
- #box(width: 3mm, height: 3mm, fill: green) positive tests that parse and negative tests that fail,
- #box(width: 3mm, height: 3mm, fill: red) positive tests that fail and negative tests that parse,
- number of tests that have a `validate` parameter, and of those:
- #box(width: 3mm, height: 3mm, fill: green) how many passed validation,
- #box(width: 3mm, height: 3mm, fill: yellow) how many were skipped due to an incorrect result in the parsing stage,
- #box(width: 3mm, height: 3mm, fill: red) and how many failed validation,

= Modularity

The first way in which KLEENE is modular is that you can access any intermediate
rules.

#test-example(```typ
#let grammar = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => (int: int(ds)))
  },
  hex: {
    pat(drop("0x"), `[0-9A-Fa-f]+`)
    rw(ds => (hex: ds))
  },
  val: {
    pat(fork(<hex>, <int>))
  },
)

#kleene.parse(grammar, <int>, "42")

#kleene.parse(grammar, <hex>, "0xDEAD")
```)

This alone is not the focus of this section, however.

== Extend <sec-extend>

```typc kleene.extend(grammar1, grammar2)``` produces a new grammar that
has the combined rules of `grammar1` and `grammar2`, in the following manner:
- rules that exist in `grammar1` but not `grammar2` are left unchanged,
- so are rules that exist in `grammar2` but not `grammar1`,
- rules that exist in both are concatenated in a way that gives `grammar1` precedence,
- unit test sets are merged.

#test-example(```typ
#let grammar1 = kleene.grammar(
  hex: {
    pat(drop("0x"), $$, `[0-9A-Fa-f]+`)
    rw(ds => (hex: ds))
    yy(`0x42`)
  },
  val: {
    pat(<hex>)
    yy(`0x42`)
  }
)

#let grammar2 = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => (int: int(ds)))
    yy(`42`)
  },
  val: {
    pat(<int>)
    yy(`42`)
  }
)

#let grammar = kleene.extend(grammar1, grammar2)
```)

Extending a grammar in this way can cause negative tests to fail.
You can use ```typc kleene.strip(grammar, yy: false)``` to remove only negative unit
tests from the extended grammar, or ```typc kleene.strip(grammar)``` to remove all unit tests.

== Patch <sec-patch>

This operation is similar to `kleene.extend`, but rather than adding to the existing
rules, ```typc kleene.patch(grammar1, grammar2)``` will keep only `grammar2`'s
version of rules that exist in both. This includes removing all unit tests from
the rules of `grammar1` that are also declared in `grammar2`.

The example below shows an application, where we have a grammar that parses
lists of integers, and we patch it into a grammar that parses lists of
booleans.

#test-example(```typ
#let grammar1 = kleene.grammar(
  int: {
    pat(`[0-9]+`)
    rw(ds => int(ds))
    yy(`42`)
  },
  comma: pat(drop(` *, *`)),
  elem: pat(<int>),
  list: {
    pat(drop("["), star(<elem>, <comma>), maybe(<elem>), drop("]"))
    rw(elts => elts.flatten())
    yy(`[]`, `[1]`, `[1,]`, `[1, 2, 3, 4]`)
  }
)

#let grammar2 = kleene.grammar(
  // <bool> is a new rule (be careful of accidental collisions)
  bool: {
    pat(fork("true", "false"))
    rw(e => ("true": true, "false": false).at(e))
    yy(`true`, `false`)
  },
  // <elem> will replace the original one
  elem: pat(<bool>),
  // We re-specify <list> but without a `pat`,
  // just to replace its unit tests that wouldn't work anymore.
  list: {
    yy(`[true, false, false]`)
  }
)

#let grammar = kleene.patch(grammar1, grammar2)
```)

= Advanced techniques

== Operator precedence

As is standard in PEGs, there is no notion of operator precedence.
This can make it more challenging to parse e.g. arithmetic expressions,
but there are standard tricks to escape this limitation.
The usual way of having a form of precedence is simply to have multiple levels
of expressions, as below:

#test-example(```typ
#let grammar = kleene.grammar(
  whitespace: {
    pat(maybe(iter(" ")))
    rw(none)
  },
  int: {
    pat(iter(`[0-9]`))
    rw(ds => int(ds.join()))
  },
  atom: {
    pat(<int>)
    rw(i => i)
    pat(drop("("), <expr>, drop(")"))
    rw((e,) => e)
    yy(`11`)
  },
  mul-expr: {
    pat(<atom>, <whitespace>, fork("*", "/"), <whitespace>, <mul-expr>)
    pat(<atom>)
    rw(auto)
    yy(`42`, `11 * 12 * 4`)
  },
  add-expr: {
    pat(<mul-expr>, <whitespace>, fork("+", "-"), <whitespace>, <add-expr>)
    pat(<mul-expr>)
    rw(auto)
    yy(`1 + 1 * 2 - 1`)
  },
  expr: {
    pat(<add-expr>)
    yy(`1 + (1 + 1) * 2`)
  },
)
```)


= Full API

#custom-type("pattern")
#custom-type("rule")
#custom-type("grammar")

== Toplevel definitions

#show-module("parse", module: "kleene")
#show-module("grammar", module: "kleene")

== Pattern combinators

#show-module("operators", module: "prelude")

== Rule definition

#show-module("builders", module: "prelude")



//#show-module("ui")

/*
= Public API <api>

These are the user-facing functions of MEANDER.

// TODO: colors
#custom-type("state")
#custom-type("elem")
#custom-type("block") // Don't forget the optional bounds and styles
#custom-type("blocks")
#custom-type("contour")
#custom-type("align")
#custom-type("size")
#custom-type("overflow")

#revised.minor[== Elements <elem-doc>]

All constructs that are valid within a ```typ #meander.reflow({ .. })``` block.
Note that they all produce an object that is a singleton dictionary,
so that the ```typ #meander.reflow({ .. })``` invocation is automatically
passed as input the concatenation of all these elements.
For clarity we use the more descriptive type @type:elem,
instead of the internal representation `(dictionary,)`

#show-module("elems")

== Layouts

These are the toplevel invocations.
They expect a sequence of `elem` as input, and produce `content`.

#show-module("layouts", module: "meander")

== Contouring <contouring-doc>

Functions for approximating non-rectangular boundaries.
We refer to those collectively as being of type @type:contour.
They can be concatenated with `+` which will apply contours successively.

#show-module("contour", module: true)

== Queries <queries>

Enables interactively fetching properties from previous elements.
See how to use them in @interactive.

#show-module("query", module: true)

== Options

Configuring the behavior of @cmd:meander:reflow.

=== Pre-layout options

These come before all elements.

==== Debug settings
Visualizing containers and obstacle boundaries.
#show-module("opt/debug", module: "opt.debug")

==== Placement settings
Controlling the interactions between content
inside and outside of the MEANDER invocation.
#show-module("opt/placement", module: "opt.placement")

=== Dynamic options

These modify parameters on the fly.

#warning-alert[None yet]

=== Post-layout options

These come after all elements.

==== Overflow settings
What happens to content that doesn't fit.
#show-module("opt/overflow", module: "opt.overflow")

== Public internals

If MEANDER is too high-level for you, you may use the public internals
made available as lower-level primitives.

#warning-alert[
  Public internal functions have a lower standard for backwards compatibility.
  Make sure to pin a specific version.
]

// @scrybe(jump import; grep {{version}})
#codesnippet[
```typ
#import "@preview/meander:0.4.1": internals.fill-box
```
]
This grants you access to the primitive `fill-box`, which is the entry
point of the content bisection algorithm. It allows you to take as much
content as fits in a specific box. See @cmd:bisect:fill-box for details.

// @scrybe(jump import; grep {{version}})
#codesnippet[
```typ
#import "@preview/meander:0.4.1": internals.geometry
```
]
This grants you access to all the functions in the `geometry` module,
which implement interesting 1D and 2D primitives. See @geometry for details.


#show-module("internals", module: true)

#revised.major[= Internal module details <internal>]

== Utils

#show-module("utils", module: true)

== Types

#show-module("types", module: true)

== Geometry <geometry>

#show-module("geometry", module: true)

== Tiling

#show-module("tiling", module: true)

== Normalization

#show-module("normalize", module: true)

== Bisection

#show-module("bisect", module: true)

== Threading

#show-module("threading", module: true)

= About

== Related works

This package takes a lot of basic ideas from
#link("https://laurmaedje.github.io/posts/layout-models/")[Typst's own builtin layout model],
mainly lifting the restriction that all containers must be of the same width,
but otherwise keeping the container-oriented workflow.
There are other tools that implement similar features, often with very different
models internally.

*In Typst:*
- #universe("wrap-it") has essentially the same output as MEANDER with
  only one obstacle and one container. It is noticeably more concise for very
  simple cases.

*In #LaTeX:*
- #link("https://ctan.org/pkg/wrapfig")[`wrapfig`] can achieve similar results
  as MEANDER as long as the images are rectangular, with the notable difference
  that it can even affect content outside of the
  ```tex \begin{wrapfigure}...\end{wrapfigure}``` environment.
- #link("https://ctan.org/pkg/floatflt")[`floatfit`] and
  #link("https://ctan.org/pkg/picins")[`picins`] can do a similar job as `wrapfig` with
  slightly different defaults.
- #link("https://ctan.org/topic/parshape")[`parshape`] is more low-level than all of
  the above, requiring every line length to be specified one at a time.
  It has the known drawback to attach to the paragraph data that depends on the
  obstacle, and is therefore very sensitive to layout adjustments.

*Others:*
- #link("https://helpx.adobe.com/indesign/using/text-wrap.html")[Adobe InDesign]
  supports threading text and wrapping around images with arbitrary shapes.

== Dependencies

In order to obtain hyphenation patterns, MEANDER imports
#universe("hy-dro-gen"), which is a wrapper around #github("typst/hypher").
This manual is built using #universe("mantys") and #universe("tidy").

== Acknowledgements

MEANDER would have taken much more effort to bootstrap had I not had access to
#universe("wrap-it")'s source code to understand the internal representation
of content, so thanks to #github-user("ntjess").

MEANDER started out as an idea in the Typst Discord server;
thanks to everyone who gave input and encouragements.

Thanks also to the people who use MEANDER and submit bug reports and feature requests,
many regressions would not have been discovered as quickly were it not for their
vigilance.
/ )
*/
